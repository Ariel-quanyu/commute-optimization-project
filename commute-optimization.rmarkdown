---
title: "Commute Optimization Project"
author: "Ariel Qian"
format:
  html:
    code-fold: true   
    code-tools: false 
    echo: false       
editor: visual
---



## 1. Project Background

Melbourne faces increasing traffic congestion, especially during peak commuting hours.
This project uses traffic sensor data from SCATS detectors to analyze vehicle volume patterns.
The goal is to identify high-congestion periods and propose data-driven recommendations for signal timing and route planning.

## 2. Data Overview

### Step 1: Load all CSVs into one dataset

## Why this step? ##
The dataset consists of multiple daily CSV files from July 2025. We load and merge them to create a complete dataset for analysis.



```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

# 加载真实数据文件夹下所有 CSV 并合并为一个 DataFrame
file_list <- list.files("traffic_signal_volume_data_july_2025", full.names = TRUE)
traffic <- file_list %>%
  map_df(read_csv)

glimpse(traffic)

```



### Step 2: Preview structure and check completeness

## Why this step? ##
I inspect the dataset’s structure to understand available variables (e.g. site ID, date, detector volume v00–v95)
and ensure all expected files were successfully loaded.



```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)

# 查看列名总览
colnames(traffic)[1:20]  # 这里只展示前 20 列，太多了

# 统计记录量与变量数
dim(traffic)
```




### Step 3: Check data consistency across days
## Why this step? ##
I visualize the number of records per day to ensure data coverage is consistent throughout July.
Sudden drops may indicate missing files or data collection issues.


```{r}
traffic %>%
  count(QT_INTERVAL_COUNT) %>%
  ggplot(aes(x = as.Date(QT_INTERVAL_COUNT), y = n)) +
  geom_col(fill = "steelblue") +
  labs(title = "Daily Record Count", x = "Date", y = "Observations")
```



## 3. Congestion Analysis

### Objective  
To identify peak congestion periods across different days and hours, helping prioritize signal timing and commute management strategies.

---

### Step 1: Calculate Total Volume per Record  
Why this step?  
The original dataset records traffic counts across multiple detector columns (`v00` to `v95`). Summing these provides the **total vehicle count** per row.



```{r, echo=FALSE, message=FALSE, warning=FALSE}
traffic <- traffic %>%
  mutate(total_volume = rowSums(select(., starts_with("v")), na.rm = TRUE))
```




### Step 2: Aggregate Daily Volume
Why this step?
Summarizing daily total volume allows us to observe overall traffic trends over time and detect any weekday-weekend patterns.



```{r, echo=FALSE, message=FALSE, warning=FALSE}


library(lubridate)

traffic_summary <- traffic %>%
  mutate(date = as.Date(QT_INTERVAL_COUNT)) %>%
  group_by(date) %>%
  summarise(daily_volume = sum(total_volume, na.rm = TRUE)) %>%
  mutate(weekday = wday(date, label = TRUE, abbr = FALSE)) 

ggplot(traffic_summary, aes(x = date, y = daily_volume)) +
  geom_line(linewidth = 1, color = "steelblue") +
  scale_x_date(
    date_labels = "%b %d\n%A",  
    date_breaks = "1 day"
  ) +
  labs(
    title = "Daily Traffic Volume Trend (July 2025)",
    x = "Date",
    y = "Total Vehicle Count"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



**Key Insight:**  
> The trend reveals a clear **weekly pattern**: traffic volumes remain high during weekdays and drop sharply over weekends.  

> This confirms that **congestion is driven by weekday commuting**, making morning and evening peak hours during those days critical for optimization.


### Step 3: Aggregate Traffic by Hour
## Why this step? ##

While daily totals help us detect weekday-weekend patterns, they do not reveal intra-day congestion spikes. To optimize signal timing and improve route planning, we need to understand which hours of the day experience the highest traffic volumes, especially during morning and evening peaks.

This step summarizes traffic by hour to identify **rush hour windows** and evaluate if there are **consistent congestion patterns** across days.



```{r}
library(tidyr)

traffic_long <- traffic %>%
  pivot_longer(
    cols = starts_with("v"),
    names_to = "interval",
    values_to = "volume"
  )

traffic_long <- traffic_long %>%
  mutate(
    date = as.Date(QT_INTERVAL_COUNT),
    interval_num = as.integer(substr(interval, 2, 3)),  # "v00" → 0, "v01" → 1, ...
    hour = floor(interval_num / 4),
    weekday = wday(date, label = TRUE, abbr = FALSE)
  )


```

```{r}

library(dplyr)
library(tidyr)
library(lubridate)
library(ggplot2)


# Step 1: 计算每个时段的总流量（如果有多个 detector 列可以加总）
traffic_long <- traffic %>%
  pivot_longer(
    cols = starts_with("v"),
    names_to = "interval",
    values_to = "volume"
  )

# Step 2: 提取日期、小时和星期
traffic_long <- traffic_long %>%
  mutate(
    date = as.Date(QT_INTERVAL_COUNT),
    interval_num = as.integer(substr(interval, 2, 3)),   # "v00" → 0, "v01" → 1, ...
    hour = floor(interval_num / 4),                     # 每4个interval为1小时
    weekday = wday(date, label = TRUE, abbr = FALSE)
  )

# Step 3: 聚合为每小时每星期的平均交通量
traffic_by_hour <- traffic_long %>%
  group_by(hour, weekday) %>%
  summarise(avg_volume = mean(volume, na.rm = TRUE), .groups = "drop")

# Step 4: 可视化
ggplot(traffic_by_hour, aes(x = hour, y = avg_volume, color = weekday, group = weekday)) +
  geom_line(linewidth = 1) +
  scale_x_continuous(breaks = 0:23) +
  labs(
    title = "Hourly Average Traffic Volume by Weekday",
    x = "Hour of Day",
    y = "Average Vehicle Count"
  ) +
  theme_minimal()


```

